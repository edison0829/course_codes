{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Generative Models for Text\n",
    "Ruotian Jiang 8389636738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a & b & c (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1624629\n",
      "Total Vocab:  100\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "data3 = open('3.txt', 'r').read()\n",
    "data4 = open('4.txt', 'r').read()\n",
    "data1 = open('1.txt', 'r').read()\n",
    "data2 = open('2.txt', 'r').read()\n",
    "data = data1 + data2 + data3 + data4\n",
    "data = data.lower()\n",
    "# here to disregard lowercase and uppercase letters.\n",
    "\n",
    "n_chars = len(data)\n",
    "n_vocab = len(set(data))\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ascii = [ord(i) for i in data]\n",
    "min_v = min(data_ascii)\n",
    "max_v = max(data_ascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c(iii) & c(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1624529\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = data_ascii[i:i + seq_length]\n",
    "    seq_out = data_ascii[i + seq_length]\n",
    "    dataX.append([j for j in seq_in])\n",
    "    dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# rescaled to [1:0]\n",
    "X = (X - min_v)/float(max_v-min_v)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c(vi) & c(vii) & c(viii) & c(ix) & c(x) \n",
    "Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section.\n",
    "Because 30 for epochs is time cosuming, about 40 hours, so I choose 20 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1624529/1624529 [==============================] - 4701s 3ms/step - loss: 2.7764\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.77635, saving model to weights-improvement-01-2.7764.hdf5\n",
      "Epoch 2/20\n",
      "1624529/1624529 [==============================] - 4707s 3ms/step - loss: 2.5714\n",
      "\n",
      "Epoch 00002: loss improved from 2.77635 to 2.57143, saving model to weights-improvement-02-2.5714.hdf5\n",
      "Epoch 3/20\n",
      "1624529/1624529 [==============================] - 4719s 3ms/step - loss: 2.4649\n",
      "\n",
      "Epoch 00003: loss improved from 2.57143 to 2.46494, saving model to weights-improvement-03-2.4649.hdf5\n",
      "Epoch 4/20\n",
      "1624529/1624529 [==============================] - 4731s 3ms/step - loss: 2.3721\n",
      "\n",
      "Epoch 00004: loss improved from 2.46494 to 2.37211, saving model to weights-improvement-04-2.3721.hdf5\n",
      "Epoch 5/20\n",
      "1624529/1624529 [==============================] - 4738s 3ms/step - loss: 2.2939\n",
      "\n",
      "Epoch 00005: loss improved from 2.37211 to 2.29392, saving model to weights-improvement-05-2.2939.hdf5\n",
      "Epoch 6/20\n",
      "1624529/1624529 [==============================] - 4740s 3ms/step - loss: 2.2326\n",
      "\n",
      "Epoch 00006: loss improved from 2.29392 to 2.23264, saving model to weights-improvement-06-2.2326.hdf5\n",
      "Epoch 7/20\n",
      "1624529/1624529 [==============================] - 4744s 3ms/step - loss: 2.1846\n",
      "\n",
      "Epoch 00007: loss improved from 2.23264 to 2.18456, saving model to weights-improvement-07-2.1846.hdf5\n",
      "Epoch 8/20\n",
      "1624529/1624529 [==============================] - 4748s 3ms/step - loss: 2.1460\n",
      "\n",
      "Epoch 00008: loss improved from 2.18456 to 2.14600, saving model to weights-improvement-08-2.1460.hdf5\n",
      "Epoch 9/20\n",
      "1624529/1624529 [==============================] - 5570s 3ms/step - loss: 2.1146\n",
      "\n",
      "Epoch 00009: loss improved from 2.14600 to 2.11457, saving model to weights-improvement-09-2.1146.hdf5\n",
      "Epoch 10/20\n",
      "1624529/1624529 [==============================] - 5091s 3ms/step - loss: 2.0873\n",
      "\n",
      "Epoch 00010: loss improved from 2.11457 to 2.08732, saving model to weights-improvement-10-2.0873.hdf5\n",
      "Epoch 11/20\n",
      "1624529/1624529 [==============================] - 5094s 3ms/step - loss: 2.0625\n",
      "\n",
      "Epoch 00011: loss improved from 2.08732 to 2.06247, saving model to weights-improvement-11-2.0625.hdf5\n",
      "Epoch 12/20\n",
      "1624529/1624529 [==============================] - 5085s 3ms/step - loss: 2.0400\n",
      "\n",
      "Epoch 00012: loss improved from 2.06247 to 2.04002, saving model to weights-improvement-12-2.0400.hdf5\n",
      "Epoch 13/20\n",
      "1624529/1624529 [==============================] - 5177s 3ms/step - loss: 2.0218\n",
      "\n",
      "Epoch 00013: loss improved from 2.04002 to 2.02182, saving model to weights-improvement-13-2.0218.hdf5\n",
      "Epoch 14/20\n",
      "1624529/1624529 [==============================] - 5140s 3ms/step - loss: 2.0043\n",
      "\n",
      "Epoch 00014: loss improved from 2.02182 to 2.00433, saving model to weights-improvement-14-2.0043.hdf5\n",
      "Epoch 15/20\n",
      "1624529/1624529 [==============================] - 5186s 3ms/step - loss: 1.9884\n",
      "\n",
      "Epoch 00015: loss improved from 2.00433 to 1.98839, saving model to weights-improvement-15-1.9884.hdf5\n",
      "Epoch 16/20\n",
      "1624529/1624529 [==============================] - 5873s 4ms/step - loss: 1.9731\n",
      "\n",
      "Epoch 00016: loss improved from 1.98839 to 1.97307, saving model to weights-improvement-16-1.9731.hdf5\n",
      "Epoch 17/20\n",
      "1624529/1624529 [==============================] - 5792s 4ms/step - loss: 1.9586\n",
      "\n",
      "Epoch 00017: loss improved from 1.97307 to 1.95856, saving model to weights-improvement-17-1.9586.hdf5\n",
      "Epoch 18/20\n",
      "1624529/1624529 [==============================] - 4898s 3ms/step - loss: 1.9471\n",
      "\n",
      "Epoch 00018: loss improved from 1.95856 to 1.94706, saving model to weights-improvement-18-1.9471.hdf5\n",
      "Epoch 19/20\n",
      "1624529/1624529 [==============================] - 4895s 3ms/step - loss: 1.9348\n",
      "\n",
      "Epoch 00019: loss improved from 1.94706 to 1.93478, saving model to weights-improvement-19-1.9348.hdf5\n",
      "Epoch 20/20\n",
      "1624529/1624529 [==============================] - 4893s 3ms/step - loss: 2.8075\n",
      "\n",
      "Epoch 00020: loss did not improve from 1.93478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124322cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  153\n",
      "Total Vocab:  27\n",
      "Total Patterns:  53\n",
      "original:\n",
      "\" ively, just as they would physical phenomena. this school of psychologists tends not to emphasize th \"\n",
      "uh \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "ihu thuh \n",
      "\n",
      "ehs \n",
      "\n",
      "phu \n",
      "\n",
      "thm \n",
      "\n",
      "ph thmhi,h��e��,e��e��e,i��h��e: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "i��h,ah thmhu,t��e��enipeni \n",
      "\n",
      "ehs \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "ah th ih th \n",
      "\n",
      "a��h��en \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a��hle \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "ais hhuhs�ivin,�� thuhs,hiu� thuhu��,\n",
      "th \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ae sh\n",
      "ih hh th \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ae sh hh th \n",
      "\n",
      "eh th \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ah\n",
      "bh nh th \n",
      "\n",
      "thu thu�ayiyie \n",
      "\n",
      "ehs \n",
      "\n",
      "ehs \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "text = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "text = text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(text)\n",
    "n_vocab = len(set(text))\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "text_ascii = [ord(i) for i in text]\n",
    "min_t = min(text_ascii)\n",
    "max_t = max(text_ascii)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text_ascii[i:i + seq_length]\n",
    "    seq_out = text_ascii[i + seq_length]\n",
    "    dataX.append([j for j in seq_in])\n",
    "    dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9348.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print \"original:\"\n",
    "print \"\\\"\", ''.join([chr(value) for value in pattern]), \"\\\"\"\n",
    "# generate characters\n",
    "signal = 0 \n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = (x - min_t)/float(max_t-min_t)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = chr(index)\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
